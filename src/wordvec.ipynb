{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9678c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Cell\n",
    "from gensim.models import KeyedVectors\n",
    "from wordfreq import word_frequency\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "281ddf13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded successfully from ./bioword.vec.bin\n"
     ]
    }
   ],
   "source": [
    "# Model Load Cell\n",
    "model = KeyedVectors.load_word2vec_format('bioword.vec.bin', binary=True)\n",
    "print(\"✅ Model loaded successfully from ./bioword.vec.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67572bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 30956 MeSH terms\n"
     ]
    }
   ],
   "source": [
    "# Load MeSH descriptor terms from XML\n",
    "mesh_path = \"desc2025.xml\"  # located in src/ alongside the notebook\n",
    "tree = ET.parse(mesh_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "# Extract MeSH descriptor terms\n",
    "mesh_terms = set()\n",
    "for record in root.findall(\".//DescriptorRecord\"):\n",
    "    term = record.findtext(\"DescriptorName/String\")\n",
    "    if term:\n",
    "        mesh_terms.add(term.lower())  # match lowercase words in BioWordVec\n",
    "\n",
    "print(f\"✅ Loaded {len(mesh_terms)} MeSH terms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c24f85c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Filtered vocab size after MeSH & wordfreq filtering: 621\n"
     ]
    }
   ],
   "source": [
    "# Filtered Vocabulary Cell\n",
    "country_blocklist = {\n",
    "    \"spain\", \"china\", \"india\", \"chile\", \"kenya\", \"ghana\", \"egypt\", \"nepal\",\n",
    "    \"niger\", \"iran\", \"iraq\", \"syria\", \"haiti\", \"italy\", \"japan\", \"qatar\",\n",
    "    \"yemen\", \"libya\", \"rwanda\", \"norway\", \"serbia\", \"sweden\", \"guinea\",\n",
    "    \"israel\", \"belize\", \"bhutan\", \"greece\", \"poland\", \"romania\", \"mexico\",\n",
    "    \"canada\", \"zambia\", \"swazis\", \"angola\", \"malta\", \"latvia\", \"togo\",\n",
    "    \"benin\", \"samoa\", \"sudan\", \"oman\", \"peru\", \"laos\", \"cuba\", \"fiji\",\n",
    "    \"chad\", \"mali\", \"tonga\", \"palau\", \"nauru\", \"yemen\", \"malta\", \"samoa\",\n",
    "    \"france\", \"brazil\", \"russia\", \"korea\", \"vietnam\", \"gabon\", \"zimbabwe\",\n",
    "    \"uganda\", \"tunisia\", \"thailand\", \"taiwan\", \"switzerland\", \"slovakia\",\n",
    "    \"slovenia\", \"singapore\", \"senegal\", \"portugal\", \"philippines\", \"pakistan\",\n",
    "    \"newzealand\", \"netherlands\", \"morocco\", \"mongolia\", \"malaysia\", \"luxembourg\",\n",
    "    \"lithuania\", \"lebanon\", \"kuwait\", \"kazakhstan\", \"jamaica\", \"indonesia\",\n",
    "    \"hungary\", \"finland\", \"estonia\", \"denmark\", \"czech\", \"croatia\", \"colombia\",\n",
    "    \"bulgaria\", \"belgium\", \"austria\", \"australia\", \"argentina\", \"algeria\", \"panama\",\n",
    "    \"europe\", \"africa\", \"asia\", \"america\", \"oceania\",\n",
    "}\n",
    "\n",
    "non_bio_terms = {\n",
    "    \"spain\", \"iran\", \"china\", \"politics\", \"crime\", \"theft\", \"war\", \"terrorism\",\n",
    "    \"economics\", \"education\", \"law\", \"insurance\", \"banking\", \"religion\", \"computers\",\n",
    "    \"marketing\", \"advertising\", \"journalism\", \"philosophy\", \"rape\", \"role\", \"jews\", \"sexism\",\n",
    "    \"fascism\", \"capitalism\", \"communism\", \"socialism\", \"feminism\", \"racism\",\n",
    "    \"homophobia\", \"transphobia\", \"xenophobia\", \"sexism\", \"ageism\", \"ableism\", \"minors\",\n",
    "    \"police\", \"violence\", \"monaco\", \"prison\", \"prisons\", \"jail\", \"jails\", \"court\", \"courts\",\n",
    "    \"judge\", \"judges\", \"lawyer\", \"lawyers\", \"attorney\", \"attorneys\", \"prosecutor\", \"theft\",\n",
    "}\n",
    "\n",
    "cities = {\n",
    "    \"london\", \"paris\", \"tokyo\", \"berlin\", \"madrid\", \"dubai\", \"delhi\", \"sydney\", \"seoul\", \"milan\",\n",
    "    \"osaka\", \"vienna\", \"zurich\", \"prague\", \"dublin\", \"miami\", \"boston\", \"chicago\", \"houston\", \"atlanta\",\n",
    "    \"toronto\", \"detroit\", \"seattle\", \"denver\", \"orlando\", \"phoenix\", \"dallas\", \"austin\", \"tampa\", \"vegas\",\n",
    "    \"lisbon\", \"munich\", \"warsaw\", \"naples\", \"brasil\", \"quito\", \"havana\", \"cairo\", \"beirut\", \"jakarta\",\n",
    "    \"manila\", \"hanoi\", \"oslo\", \"sofia\", \"riga\", \"varna\", \"geneva\", \"brno\", \"porto\", \"malaga\",\n",
    "    \"nice\", \"lyon\", \"crete\", \"varna\", \"goa\", \"pune\", \"kyoto\", \"nagoya\", \"kobe\", \"sapporo\"\n",
    "}\n",
    "\n",
    "us_states = {\n",
    "    \"alabama\", \"alaska\", \"arizona\", \"arkansas\", \"california\", \"colorado\",\n",
    "    \"connecticut\", \"delaware\", \"florida\", \"georgia\", \"hawaii\", \"idaho\",\n",
    "    \"illinois\", \"indiana\", \"iowa\", \"kansas\", \"kentucky\", \"louisiana\",\n",
    "    \"maine\", \"maryland\", \"massachusetts\", \"michigan\", \"minnesota\",\n",
    "    \"mississippi\", \"missouri\", \"montana\", \"nebraska\", \"nevada\",\n",
    "    \"newhampshire\", \"newjersey\", \"newmexico\", \"newyork\",\n",
    "    \"northcarolina\", \"northdakota\", \"ohio\", \"oklahoma\",\n",
    "    \"oregon\", \"pennsylvania\", \"rhodeisland\", \"southcarolina\", \"southdakota\", \"tennessee\", \"texas\", \n",
    "    \"utah\", \"vermont\", \"virginia\", \"washington\", \"westvirginia\", \"wisconsin\", \"wyoming\"\n",
    "}\n",
    "\n",
    "colors = {\n",
    "    \"red\", \"blue\", \"green\", \"black\", \"white\", \"yellow\", \"purple\", \"orange\", \"violet\", \"indigo\", \"silver\", \"golden\", \"bronze\"\n",
    "}\n",
    "\n",
    "# Make a Set out of all the sets above\n",
    "blocklist = set()\n",
    "blocklist.update(country_blocklist)\n",
    "blocklist.update(non_bio_terms)\n",
    "blocklist.update(cities)\n",
    "blocklist.update(us_states)\n",
    "blocklist.update(colors)\n",
    "\n",
    "def bio_score(word):\n",
    "    bio_sim = model.similarity(word, \"biology\")\n",
    "\n",
    "    life_sim = model.similarity(word, \"life\") ** 1.5\n",
    "    cell_sim = model.similarity(word, \"cell\") ** 1.5\n",
    "    chem_sim = model.similarity(word, \"chemistry\") ** 1.5\n",
    "    disease_sim = model.similarity(word, \"disease\") ** 1.5\n",
    "    org_sim = model.similarity(word, \"organism\") ** 1.5\n",
    "    animal_sim = model.similarity(word, \"animal\") ** 1.5\n",
    "    plant_sim = model.similarity(word, \"plant\") ** 1.5\n",
    "    microbe_sim = model.similarity(word, \"microbe\") ** 1.5\n",
    "    anatomy_sim = model.similarity(word, \"anatomy\") ** 1.5\n",
    "    human_sim = model.similarity(word, \"human\") ** 1.5\n",
    "    \n",
    "    other_sims = sorted([life_sim, cell_sim, chem_sim, disease_sim, org_sim, animal_sim, plant_sim, microbe_sim, anatomy_sim, human_sim], reverse=True)[:7]\n",
    "    similarity = (2 * bio_sim + sum(other_sims)) / 10\n",
    "    return round(similarity, 4)\n",
    "\n",
    "def is_clean_word(w):\n",
    "    return (\n",
    "        4 <= len(w) <= 6\n",
    "        and w.isalpha()\n",
    "        and re.match(r\"^[a-z]{4,6}$\", w)\n",
    "        and any(c in 'aeiou' for c in w)\n",
    "        and w not in blocklist\n",
    "        and word_frequency(w, 'en') < 1e-3\n",
    "        and word_frequency(w, 'en') > 1e-8\n",
    "        and w in mesh_terms\n",
    "        and bio_score(w) > 0.16\n",
    "    )\n",
    "\n",
    "filtered_vocab_list = [w for w in model.key_to_index if is_clean_word(w)]\n",
    "bio_words = set(filtered_vocab_list)\n",
    "print(f\"✅ Filtered vocab size after MeSH & wordfreq filtering: {len(bio_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025e3da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 200 words from the filtered vocabulary\n",
    "sampled_words = random.sample(filtered_vocab_list, 400)\n",
    "\n",
    "# Print the words in a 20x20 grid\n",
    "for i in range(0, 400, 20):\n",
    "    print(\"\\t\".join(sampled_words[i:i+20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c9c6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total number of bio words: 767\n"
     ]
    }
   ],
   "source": [
    "# extras del chac...\n",
    "extra_bio_words = {\n",
    "    'gout', 'myopia', 'mitogen', 'domain', 'enzyme', 'corvus', 'anole',\n",
    "    'zebra', 'estrus', 'krait', 'goat', 'embryo', 'spine', 'indri',\n",
    "    'tonsil', 'swine', 'baleen', 'nucleus', 'apical', 'hepatic',\n",
    "    'minnow', 'cactus', 'olive', 'capsule', 'deer', 'lion', 'trunk',\n",
    "    'uterus', 'rabies', 'gaur', 'leptin', 'aorta', 'rana', 'clone',\n",
    "    'pyrus', 'rice', 'mussel', 'melanin', 'nyala', 'palea', 'moss',\n",
    "    'fever', 'cicada', 'axil', 'taiga', 'carapace', 'cervus', 'siren',\n",
    "    'sucrose', 'octopus', 'larva', 'mange', 'dingo', 'sponge', 'bract',\n",
    "    'carotene', 'capsid', 'viper', 'claw', 'cornea', 'pupa', 'whale',\n",
    "    'toxin', 'lobster', 'helix', 'codon', 'catfish', 'phloem', 'soma',\n",
    "    'cuticle', 'equine', 'redox', 'saliva', 'molar', 'klippe', 'polyp',\n",
    "    'fly', 'gable', 'mollusk', 'sapling', 'grotto', 'sapiens', 'puffer',\n",
    "    'zonation', 'larvae', 'fungus', 'cytosol', 'retina', 'ribose', 'cough'\n",
    "}\n",
    "\n",
    "other_bio_words = {\n",
    "    'absorb', 'acari', 'algaes', 'amoeba', 'amylas', 'anther', 'angios', 'archae',\n",
    "    'asthma', 'bamboo', 'bacter', 'beetle', 'bronch', 'cancer', 'cholera', 'collag',\n",
    "    'corn', 'cortex', 'cotyle', 'cyto', 'cytosol', 'dopami', 'ebola', 'embryo',\n",
    "    'enzyme', 'euglen', 'fever', 'ferns', 'filter', 'flower', 'flagel', 'fungi',\n",
    "    'fungus', 'fusion', 'giardi', 'glands', 'glucos', 'golgi', 'herpes', 'hormone',\n",
    "    'horse', 'human', 'induce', 'insect', 'insuli', 'kinase', 'lipid', 'lipids',\n",
    "    'malari', 'mammal', 'mango', 'measle', 'meiosi', 'mitosi', 'monera',\n",
    "    'monkey', 'moss', 'mouse', 'muscle', 'neuron', 'nucleus', 'onions', 'orange',\n",
    "    'osmosis', 'ovary', 'parrot', 'patella', 'petals', 'phloem', 'pollen',\n",
    "    'primate', 'quorum', 'rabbit', 'rabies', 'reptil', 'replic', 'retina', 'ribose',\n",
    "    'rodent', 'rotavi', 'snail', 'sepals', 'spiroc', 'tibia', 'tomato', 'toxopl',\n",
    "    'toxin', 'tree', 'tulip', 'urease', 'ureter', 'vesicl', 'villi', 'virus', 'whale',\n",
    "    'xylem', 'zebra'\n",
    "}\n",
    "\n",
    "# Add extra words to the bio_words set\n",
    "bio_words.update(extra_bio_words)\n",
    "bio_words.update(other_bio_words)\n",
    "\n",
    "# Print the total number of bio words\n",
    "print(f\"✅ Total number of bio words: {len(bio_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e1cf6e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcribe to root/assets/data/bio_words.txt\n",
    "output_dir = Path(\"../assets/data\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_file = output_dir / \"bio_words.txt\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    for word in bio_words:\n",
    "        f.write(f\"{word}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
